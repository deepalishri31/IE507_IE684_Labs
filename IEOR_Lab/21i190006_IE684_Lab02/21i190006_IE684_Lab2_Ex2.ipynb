{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "21i190006_IE684_Lab2_Ex2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 2:"
      ],
      "metadata": {
        "id": "YzEbMB48YEno"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVkab74DJsRL"
      },
      "source": [
        "Recall that we implemented the gradient descent algorithm to solve $\\min_{\\mathbf{x} \\in {\\mathbb{R}}^n} f(\\mathbf{x})$. The main ingredients in the gradient descent iterations are the descent direction $\\mathbf{p}^k$ which is set to $-\\nabla f(\\mathbf{x}^k)$, and the step length $\\eta^k$ which is found by solving an optimization problem (or sometimes taken as a constant value over all iterations). We used the following procedure in the previous lab:\n",
        "\n",
        "\\begin{align}\n",
        "& \\textbf{Input:} \\text{ Starting point $x^0$, Stopping tolerance $\\tau$}  \\\\\n",
        "& \\textbf{Initialize } k=0 \\\\ \n",
        "& \\mathbf{p}^k =-\\nabla f(\\mathbf{x}^k) \\\\ \n",
        "&\\textbf{While } \\| \\mathbf{p}^k \\|_2 > \\tau \\text{ do:}  \\\\   \n",
        "&\\quad \\quad \\eta^k = \\arg\\min_{\\eta\\geq 0} f(\\mathbf{x}^k + \\eta  \\mathbf{p}^k) = \\arg\\min_{\\eta\\geq 0} f(\\mathbf{x}^k - \\eta  \\nabla f(\\mathbf{x}^k)) \\\\\n",
        "&\\quad \\quad \\mathbf{x}^{k+1} = \\mathbf{x}^k + \\eta^k \\mathbf{p}^k = \\mathbf{x}^k - \\eta^k \\nabla f (\\mathbf{x}^k)  \\\\ \n",
        "&\\quad \\quad k = {k+1} \\\\ \n",
        "&\\textbf{End While} \\\\\n",
        "&\\textbf{Output: } \\mathbf{x}^k\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7ivDCuJRP9b"
      },
      "source": [
        "We saw that for particular cases of quadratic functions, a closed form analytical solution for the minimizer of the optimization problem $\\min_{\\eta \\geq 0} f({\\mathbf{x}}^k + \\eta {\\mathbf{p}}^k)$ exists. However finding a closed form expression as a solution to this optimization problem to find a suitable step length might not always be possible. To tackle general situations, we will try to devise a different procedure in this lab. \n",
        "\n",
        "To find the step length, we will use the following property: \n",
        "Suppose a non-zero $\\mathbf{p} \\in {\\mathbb{R}}^n$ is a descent direction at point $\\mathbf{x}$, and let $\\gamma \\in (0,1)$. Then there exists $\\varepsilon >0$ such that  \n",
        "\\begin{align}\n",
        "f(\\mathbf{x}+\\alpha \\mathbf{p}) \\leq f(\\mathbf{x}) + \\gamma \\alpha \\nabla f(\\mathbf{x})^\\top \\mathbf{p}, \\ \\forall \\alpha \\in (0,\\varepsilon].  \n",
        "\\end{align}\n",
        "\n",
        "The step length $\\eta^k$ can be found using a backtracking procedure illustrated below to find appropriate value of $\\varepsilon$.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV6OddaNAmpA"
      },
      "source": [
        "\n",
        "\\begin{align}\n",
        "& \\textbf{Input:}  \\text{ $\\mathbf{x}^k$, $\\mathbf{p}^k$, $\\alpha^0$, $\\rho \\in (0,1)$, $\\gamma \\in (0,1)$ }  \\\\\n",
        "& \\textbf{Initialize } \\alpha=\\alpha^0 \\\\ \n",
        "&\\textbf{While } f(\\mathbf{x}^k + \\alpha \\mathbf{p}^k)   > f(\\mathbf{x}^k) + \\gamma \\alpha \\nabla f(\\mathbf{x}^k)^\\top \\mathbf{p}^k \\text{ do:}  \\\\   \n",
        "&\\quad \\quad \\alpha = \\rho \\alpha  \\\\\n",
        "&\\textbf{End While} \\\\\n",
        "&\\textbf{Output: } \\alpha\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW-xcDISWmGR"
      },
      "source": [
        "In this exercise, we will check if finding the steplength using the backtracking procedure is advantageous for some quadratic functions. In this sample code we consider $f(\\mathbf{x})=f(x_1,x_2,...,x_N) = (x_1-2)^2)/8 + (x_2-2^2)^2/8^2 +...+(x_N-2^N)/8^N$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJq7tIgIRroP"
      },
      "source": [
        "#numpy package will be used for most of our lab exercises. Please have a look at Please have a look at https://numpy.org/doc/stable/ for numpy documentation\n",
        "#we will first import the numpy package and name it as np\n",
        "import numpy as np \n",
        "#Henceforth, we can lazily use np to denote the much longer numpy !! "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZjX2IwOR8_X"
      },
      "source": [
        "#Now we will define a function which will compute and return the function value \n",
        "def evalf(x):  \n",
        "  #Input: x is a numpy array of size 3 \n",
        "  assert type(x) is np.ndarray and len(x) == 3 #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the objective function value\n",
        "  #compute the function value and return it \n",
        "  return (x[0]-2)**2/8 + (x[1]-4)**2/64+(x[2]-8)**2/512\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6klpwtDra_I8"
      },
      "source": [
        "#Now we will define a function which will compute and return the gradient value as a numpy array \n",
        "def evalg(x):  \n",
        "  #Input: x is a numpy array of size 3\n",
        "  assert type(x) is np.ndarray and len(x) == 3 #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the gradient value\n",
        "  #compute the gradient value and return it \n",
        "  return np.array([(x[0]-2)/4, (x[1]-4)/32,(x[2]-8)/256])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3blM08V0HOl"
      },
      "source": [
        "#Complete the module to compute the steplength by using the closed-form expression\n",
        "def compute_steplength_exact(gradf, A): #add appropriate arguments to the function \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 3 \n",
        "  assert type(A) is np.ndarray and A.shape[0] == 3 and  A.shape[1] == 3 #allow only a 3x3 array\n",
        "   \n",
        "  #Complete the code to compute step length\n",
        "  step_length = np.dot(gradf,gradf)/np.dot(gradf,np.matmul(2*A,gradf))\n",
        "\n",
        "  \n",
        "  return step_length"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGunDYy6Q21S"
      },
      "source": [
        "#Complete the module to compute the steplength by using the backtracking line search\n",
        "def compute_steplength_backtracking(x, gradf, alpha_start, rho, gamma): #add appropriate arguments to the function \n",
        "  assert type(x) is np.ndarray and len(gradf) == 3 \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 3 \n",
        "  \n",
        "  alpha = alpha_start\n",
        "  p=-evalg(x)\n",
        "  #implement the backtracking line search\n",
        "  while evalf(x+alpha*p)>evalf(x)+gamma*alpha*np.dot(evalg(x),p):\n",
        "    alpha = rho*alpha\n",
        "\n",
        "  #print('final step length:',alpha)\n",
        "  return alpha"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaUUdzLtVSCl"
      },
      "source": [
        "#we define the types of line search methods that we have implemented\n",
        "EXACT_LINE_SEARCH = 1\n",
        "BACKTRACKING_LINE_SEARCH = 2\n",
        "CONSTANT_STEP_LENGTH = 3"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SCJdqivdpxx"
      },
      "source": [
        "def find_minimizer(start_x, tol, line_search_type, *args):\n",
        "  #Input: start_x is a numpy array of size 3, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 3 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  # construct a suitable A matrix for the quadratic function \n",
        "  A = np.array([[1/8, 0 , 0],[0,1/64,0],[0,0,1/8**3]])\n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "\n",
        "  #initialization for backtracking line search\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\n",
        "    alpha_start = args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        "    #print('Params for Backtracking LS: alpha start:', alpha_start, 'rho:', rho,' gamma:', gamma)\n",
        "\n",
        "  k = 0\n",
        "  #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "  \n",
        "    if line_search_type == EXACT_LINE_SEARCH:\n",
        "      step_length = compute_steplength_exact(g_x, A) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('EXACT LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      step_length = compute_steplength_backtracking(x,g_x, alpha_start,rho, gamma) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('BACKTRACKING LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\n",
        "      step_length = 0.1\n",
        "    else:  \n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "    \n",
        "    #implement the gradient descent steps here   \n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "\n",
        "    #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  return x , k , evalf(x) \n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 2."
      ],
      "metadata": {
        "id": "EsfDcvrqI5qm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-kHCkbwe-M4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab2affd7-5767-4173-e24c-5cb21aec0f59"
      },
      "source": [
        "my_start_x = np.array([1,1,1])\n",
        "my_tol= 1e-5\n",
        "\n",
        "x_opt , k , minimum = find_minimizer(my_start_x, my_tol, CONSTANT_STEP_LENGTH)\n",
        "print(\"Final minimizer for constant step length is :\",x_opt)\n",
        "print(\"Number of iterations are :\",k)\n",
        "print(\"Minimum function value is :\",minimum)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final minimizer for constant step length is : [2.         4.         7.99744099]\n",
            "Number of iterations are : 20256\n",
            "Minimum function value is : 1.2790080180472962e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcsCIAntMNdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b396628-961d-4900-e8b3-8a2a56e5e120"
      },
      "source": [
        "#check what happens when you call find_minimzer using backtracking line search\n",
        "x_opt_bls , k , minimum = find_minimizer(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, 1, 0.5,0.5)\n",
        "print(\"Final minimizer for backtracking line search is :\",x_opt_bls)\n",
        "print(\"Number of iterations are :\",k)\n",
        "print(\"Minimum function value is :\",minimum)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final minimizer for backtracking line search is : [2.         4.         7.99744063]\n",
            "Number of iterations are : 2022\n",
            "Minimum function value is : 1.2793697352715871e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 3."
      ],
      "metadata": {
        "id": "jd4eAPltJLul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tolerance = 10**(-10)\n",
        "start_x  = np.array([1/64,1/8,1])\n",
        "x_opt , k , minimum = find_minimizer(start_x, tolerance, EXACT_LINE_SEARCH)\n",
        "print(\"Number of iterations using exact line search :\",k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovAoGEafIfdb",
        "outputId": "44f2b9f1-5171-4ea8-d180-c45af150c535"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of iterations using exact line search : 269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_opt_bls , k , minimum = find_minimizer(start_x, tolerance, BACKTRACKING_LINE_SEARCH, 1, 0.5,0.5)\n",
        "print(\"Number of iterations using backtracking line search :\",k)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyv3ZtciJ3vM",
        "outputId": "cd2d7ff7-71ad-4a71-c6df-df8c0251b31e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of iterations using backtracking line search : 4964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation :\n",
        "Number of iterations using backtracking line search(4964) method are very much higher than the number of iterations in exact line search method (269)"
      ],
      "metadata": {
        "id": "PHUL4sMsKfDY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 4."
      ],
      "metadata": {
        "id": "EewTlSccLBEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we will define a function which will compute and return the function value \n",
        "def evalf(x):  \n",
        "  #Input: x is a numpy array of size 4 \n",
        "  assert type(x) is np.ndarray and len(x) == 4 #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the objective function value\n",
        "  #compute the function value and return it \n",
        "  return (x[0]-2)**2/8 + (x[1]-4)**2/64+(x[2]-8)**2/512 +(x[3]-16)**2/(8**4)"
      ],
      "metadata": {
        "id": "byy5uruyeiC-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we will define a function which will compute and return the gradient value as a numpy array \n",
        "def evalg(x):  \n",
        "  #Input: x is a numpy array of size 4\n",
        "  assert type(x) is np.ndarray and len(x) == 4 #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the gradient value\n",
        "  #compute the gradient value and return it \n",
        "  return np.array([(x[0]-2)/4, (x[1]-4)/32,(x[2]-8)/256,2*(x[3]-16)/(8**4)])"
      ],
      "metadata": {
        "id": "GmH9iZbhfbYK"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Complete the module to compute the steplength by using the closed-form expression\n",
        "def compute_steplength_exact(gradf, A): #add appropriate arguments to the function \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 4 \n",
        "  assert type(A) is np.ndarray and A.shape[0] == 4 and  A.shape[1] == 4 #allow only a 4x4 array\n",
        "   \n",
        "  #Complete the code to compute step length\n",
        "  step_length = np.dot(gradf,gradf)/np.dot(gradf,np.matmul(2*A,gradf))\n",
        "\n",
        "  \n",
        "  return step_length"
      ],
      "metadata": {
        "id": "jv678zc0ge9H"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Complete the module to compute the steplength by using the backtracking line search\n",
        "def compute_steplength_backtracking(x, gradf, alpha_start, rho, gamma): #add appropriate arguments to the function \n",
        "  assert type(x) is np.ndarray and len(gradf) == 4\n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 4\n",
        "  \n",
        "  alpha = alpha_start\n",
        "  p=-evalg(x)\n",
        "  #implement the backtracking line search\n",
        "  while evalf(x+alpha*p)>evalf(x)+gamma*alpha*np.dot(evalg(x),p):\n",
        "    alpha = rho*alpha\n",
        "\n",
        "  #print('final step length:',alpha)\n",
        "  return alpha"
      ],
      "metadata": {
        "id": "5eft2Ezcg21b"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we define the types of line search methods that we have implemented\n",
        "EXACT_LINE_SEARCH = 1\n",
        "BACKTRACKING_LINE_SEARCH = 2\n",
        "CONSTANT_STEP_LENGTH = 3"
      ],
      "metadata": {
        "id": "LI-weFhLg_0E"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_minimizer(start_x, tol, line_search_type, *args):\n",
        "  #Input: start_x is a numpy array of size 4, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 4 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  # construct a suitable A matrix for the quadratic function \n",
        "  A = np.array([[1/8, 0 , 0 ,0],[0,1/64,0,0],[0,0,1/8**3,0],[0,0,0,1/8**4]])\n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "\n",
        "  #initialization for backtracking line search\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\n",
        "    alpha_start = args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        "    #print('Params for Backtracking LS: alpha start:', alpha_start, 'rho:', rho,' gamma:', gamma)\n",
        "\n",
        "  k = 0\n",
        "  #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "  \n",
        "    if line_search_type == EXACT_LINE_SEARCH:\n",
        "      step_length = compute_steplength_exact(g_x, A) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('EXACT LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      step_length = compute_steplength_backtracking(x,g_x, alpha_start,rho, gamma) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('BACKTRACKING LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\n",
        "      step_length = 0.1\n",
        "    else:  \n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "    \n",
        "    #implement the gradient descent steps here   \n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "\n",
        "    #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  return x , k , evalf(x) "
      ],
      "metadata": {
        "id": "ZyqsmJxfhFBc"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tolerance = 10**(-10)\n",
        "start_x  = np.array([1/512,1/64,1/8,1])\n",
        "x_opt , k , minimum = find_minimizer(start_x, tolerance, EXACT_LINE_SEARCH)\n",
        "print(\"Number of iterations using exact line search :\",k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoBXFN_hh_k5",
        "outputId": "03da7bee-2713-4c69-f833-0f17583ba3d4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of iterations using exact line search : 2013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_opt_bls , k , minimum = find_minimizer(start_x, tolerance, BACKTRACKING_LINE_SEARCH, 1, 0.5,0.5)\n",
        "print(\"Number of iterations using backtracking line search :\",k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEokuAxyiDmF",
        "outputId": "9d16c17a-a79b-4cbd-be5c-04d307855280"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of iterations using backtracking line search : 37079\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation : \\\\\n",
        "Yes similar observations hold for N=4 case ,here also the number of iterations using backtracking line search(37079) are very much greater than those using exact line search(2013)"
      ],
      "metadata": {
        "id": "OdgmiebHimSA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 5."
      ],
      "metadata": {
        "id": "_KAazqZZjLdH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The similar observations should be seen for N>4 case also that means the back tracking line search method will take more iterations than the exact line search method to minimize the function and hence the time taken to minimize the function will also be in proportion to the number of iterations.However the minimum value obtained using backtracking line search will always be little bit more efficient than that obtained using exact line search method."
      ],
      "metadata": {
        "id": "lNpxTdnPP1t2"
      }
    }
  ]
}