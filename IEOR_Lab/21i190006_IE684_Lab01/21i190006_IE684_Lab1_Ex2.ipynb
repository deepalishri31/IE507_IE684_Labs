{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "21i190006_IE684_Lab1_Ex2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVE0Xoa0Q5wE"
      },
      "source": [
        "$\\Large\\textbf{Lab 1. Exercise 2. }$\n",
        "\n",
        "Now we will consider a slightly different algorithm which can be used to find a minimizer of the function $f(\\mathbf{x})=f(x_1,x_2)= (x_1+100)^2 + (x_2-25)^2$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gpe6eGRLvSh"
      },
      "source": [
        "$\\textbf{Ques 1}$ Write the function $f(\\mathbf{x})$ in the form $\\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} + 2 \\mathbf{b}^\\top \\mathbf{x} + c$, where $\\mathbf{x}\\in {\\mathbb{R}}^2$, $\\mathbf{A}$ is a symmetric matrix of size $2 \\times 2$, $\\mathbf{b}\\in{\\mathbb{R}}^2$ and $c\\in\\mathbb{R}$. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTPeLBt0L7F7"
      },
      "source": [
        "Write your answer here:\n",
        "\n",
        "$f(\\mathbf{x})=f(x_1,x_2)= (x_1+100)^2 + (x_2-25)^2 \n",
        "= x_1^2 + x_2^2 + 200x_1 -50x_2 + 10625$\n",
        "\n",
        "So we take:\n",
        "\n",
        " $ \\mathbf{A} $  $ = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1\\end{bmatrix}\n",
        "$\n",
        "\n",
        " $ \\mathbf{x} =  \\begin{bmatrix} x_1 \\\\ x_2\\end{bmatrix} $\n",
        " \n",
        " $ \\mathbf{b} = \\begin{bmatrix} 100 \\\\ -25 \\end{bmatrix}$\n",
        "\n",
        " $ \\mathbf{c} = 10625 $\n",
        "\n",
        " And so the function $f(\\mathbf{x})$ can be written in the form $\\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} + 2 \\mathbf{b}^\\top \\mathbf{x} + c$, where $\\mathbf{x}\\in {\\mathbb{R}}^2$, $\\mathbf{A}$ is a symmetric matrix of size $2 \\times 2$, $\\mathbf{b}\\in{\\mathbb{R}}^2$ and $c\\in\\mathbb{R}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjANnIQ3L39D"
      },
      "source": [
        "\n",
        "$\\textbf{Ques 2:}$ It turns out that for a function $f:{\\mathbb{R}}^n\\rightarrow \\mathbb{R}$ of the form $f(\\mathbf{x})=\\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} + 2 \\mathbf{b}^\\top \\mathbf{x} + c$, where $\\mathbf{A}\\in{\\mathbb{R}}^{n \\times n}$ is a symmetric matrix, $\\mathbf{b} \\in {\\mathbb{R}}^n$ and $c\\in \\mathbb{R}$, the analytical solution to $\\min_{\\alpha \\geq 0} f(\\mathbf{x} - \\alpha \\nabla f(\\mathbf{x}))$ can be found in closed form. Find the solution. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jU-adJ0L-P1"
      },
      "source": [
        "Write your answer here:\n",
        "\n",
        "We can write $f(x)$ as $f(\\mathbf{x})=\\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} + 2 \\mathbf{b}^\\top \\mathbf{x} + c$, where $\\mathbf{A}\\in{\\mathbb{R}}^{n \\times n}$ is a symmetric matrix, $\\mathbf{b} \\in {\\mathbb{R}}^n$ and $c\\in \\mathbb{R}$.\n",
        "\n",
        "according to question , $$x-α∇f(x)= \\begin{bmatrix}\n",
        "x_1\\\\\n",
        "x_2\n",
        "\\end{bmatrix}-\\alpha\\begin{bmatrix}\n",
        "2(x_1+100)\\\\\n",
        "2(x_2-25)\n",
        "\\end{bmatrix}\\\\\n",
        "⇒\\begin{bmatrix}\n",
        "x_1(1-2\\alpha)-200\\alpha\\\\\n",
        "x_2(1-2\\alpha)+50\\alpha\n",
        "\\end{bmatrix}\\\\\n",
        "$$\n",
        "Now $f(x-α∇f(x)) = $ $=>f(\\begin{bmatrix}\n",
        "x_1(1-2\\alpha)-200\\alpha\\\\\n",
        "x_2(1-2\\alpha)+50\\alpha\n",
        "\\end{bmatrix})= (x_1(1-2α)-200\\alpha+100)^2+(x_2(1-2α)+25(2\\alpha-1)^2$\n",
        "\n",
        "For minimum ,\n",
        "\n",
        "we will differentiate  $ f(\\mathbf{x} - \\alpha \\nabla f(\\mathbf{x}))$ with respect to $α$\n",
        " and  put the derivative equal to 0.\n",
        "\n",
        "By differenciating , we get zero at ⍺= 0.5\n",
        "\n",
        "So, $f(\\mathbf{x} - \\alpha \\nabla f(\\mathbf{x})$ will be minimum at α=0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVkab74DJsRL"
      },
      "source": [
        "We will use this idea to construct a suitable step length finding procedure for our modified algorithm given below: \n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "& \\textbf{Input:} \\text{ Starting point $x^0$, Stopping tolerance $\\tau$}  \\\\\n",
        "& \\textbf{Initialize } k=0 \\\\ \n",
        "&\\textbf{While } \\| \\nabla f(\\mathbf{x}^k) \\|_2 > \\tau \\text{ do:}  \\\\   \n",
        "&\\quad \\quad \\eta^k = \\arg\\min_{\\eta\\geq 0} f(\\mathbf{x}^k - \\eta  \\nabla f(\\mathbf{x}^k)) \\\\\n",
        "&\\quad \\quad \\mathbf{x}^{k+1} \\leftarrow \\mathbf{x}^k - \\eta^k \\nabla f(\\mathbf{x}^k)  \\\\ \n",
        "&\\quad \\quad k = {k+1} \\\\ \n",
        "&\\textbf{End While} \\\\\n",
        "&\\textbf{Output: } \\mathbf{x}^k\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJq7tIgIRroP"
      },
      "source": [
        "#numpy package will be used for most of our lab exercises. Please have a look at https://numpy.org/doc/stable/ for numpy documentation\n",
        "#we will first import the numpy package and name it as np\n",
        "import numpy as np \n",
        "#Henceforth, we can lazily use np to denote the much longer numpy !! "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZjX2IwOR8_X"
      },
      "source": [
        "#Now we will define a function which will compute and return the function value \n",
        "def evalf(x):  \n",
        "  #Input: x is a numpy array of size 2 \n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the objective function value\n",
        "  return (x[0]+100)**2 + (x[1]-25)**2\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6klpwtDra_I8"
      },
      "source": [
        "#Now we will define a function which will compute and return the gradient value as a numpy array \n",
        "def evalg(x):  \n",
        "  #Input: x is a numpy array of size 2 \n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the gradient value\n",
        "  return np.array([2*(x[0]+100),2*(x[1]-25)])"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3blM08V0HOl"
      },
      "source": [
        "#Completing the module to compute the steplength\n",
        "def compute_steplength(initial_point,gradient_x): #add appropriate arguments to the function \n",
        "  assert type(initial_point) is np.ndarray and len(initial_point)==2\n",
        "  assert type(gradient_x) is np.ndarray and len(gradient_x) == 2\n",
        "  step_length = 1\n",
        "  c=0.0001\n",
        "  rho=0.5\n",
        "  u=c*step_length*(np.linalg.multi_dot([gradient_x,gradient_x]))\n",
        "  while evalf(initial_point - step_length*gradient_x)>evalf(initial_point)-u:\n",
        "    step_length = rho*step_length\n",
        "  return step_length"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SCJdqivdpxx"
      },
      "source": [
        "def find_minimizer(start_x, tol):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "  k = 0\n",
        "  print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "    step_length = compute_steplength(x,g_x) #call the new function you wrote to compute the steplength\n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "    print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  return x ,k\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-kHCkbwe-M4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8187db9-68ed-46e3-f9c0-fce208b9e03a"
      },
      "source": [
        "my_start_x = np.array([10,10])\n",
        "my_tol= 1e-3\n",
        "find_minimizer(my_start_x, my_tol)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([-100.,   25.]), 1)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 4:"
      ],
      "metadata": {
        "id": "fZyxriWcs_So"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_x=np.array([10,10])\n",
        "Tolerance_values=[10**(-p) for p in range(1,11,1)] "
      ],
      "metadata": {
        "id": "unh_U-WitBV0"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Num_of_iterations=[]\n",
        "for i in range(10):\n",
        "  final_minimizer,num_iter = find_minimizer(start_x,Tolerance_values[i])\n",
        "  Num_of_iterations.append(num_iter)\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDD7Z5yJtoGI",
        "outputId": "0971f489-688a-441f-fa1a-3aa199bc7287"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(Tolerance_values,Num_of_iterations)\n",
        "plt.xlabel(\"Tolerance values\")\n",
        "plt.ylabel('Number of iterations')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "MibqeDChuoI2",
        "outputId": "3c4cae65-c8d9-43f2-ad9b-ad7be2ccedd8"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation : \\\\\n",
        "In Ex 1 we have seen that when the step length is constant the for the lower tolerance values we saw that we had more number of iterations but now when the step length is not constant we have fix number of iterations"
      ],
      "metadata": {
        "id": "G4JjK0wlvxPy"
      }
    }
  ]
}