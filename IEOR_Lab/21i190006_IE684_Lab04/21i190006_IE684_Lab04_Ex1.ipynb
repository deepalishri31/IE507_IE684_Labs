{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "21i190006_IE684_Lab04_Ex1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vofEhXZupaQc"
      },
      "source": [
        "$\\large\\textbf{Exercise 1.}$\\\n",
        "The given function is :: $f(x) = 400* x_{1}^2 + 0.004x_{2}^{4}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ni-55h6hCq3"
      },
      "source": [
        "import numpy as np "
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DD5MKQ7ch7-h"
      },
      "source": [
        "def evalf(x):  \n",
        "  assert type(x) is np.ndarray and len(x) == 2\n",
        "  return 400*(x[0]**2) + 0.004*(x[1])**4"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYyKA6TsjKq5"
      },
      "source": [
        "def evalg(x):  \n",
        "  assert type(x) is np.ndarray and len(x) == 2\n",
        "  #after checking if the argument is valid, we can compute the gradient value\n",
        "  #compute the gradient value and return it \n",
        "  return np.array([800*x[0], 4*0.004* (x[1]**3)])"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gCuFWzCiP0D"
      },
      "source": [
        "def evalh(x): #Hessian matrix\n",
        "  assert type(x) is np.ndarray and len(x) == 2\n",
        "  return np.array([[800.0, 0.0], [0.0, 12*0.004*(x[1]**2)]])"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_D_k(x):\n",
        "  assert type(x) is np.ndarray\n",
        "  assert len(x) == 2\n",
        "  #compute and return D_k\n",
        "  hess = evalh(x)\n",
        "  return np.array([[1/hess[0][0],0],[0, 1/hess[1][1]]])"
      ],
      "metadata": {
        "id": "4aNVqnPpEM4d"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGunDYy6Q21S"
      },
      "source": [
        "def compute_steplength_backtracking(x, gradf, alpha_start, rho, gamma):\n",
        "  assert type(x) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(alpha_start) is float and alpha_start>=0. \n",
        "  assert type(rho) is float and rho>=0.\n",
        "  assert type(gamma) is float and gamma>=0. \n",
        "\n",
        "  alpha = alpha_start\n",
        "  p = -gradf\n",
        "  #implement the backtracking line search\n",
        "  while evalf(x + alpha*p) > (evalf(x)-(gamma*alpha*np.dot(p,p))):\n",
        "    #while evalf(x + alpha*p) > evalf(x) + gamma * alpha* (np.matmul(np.matrix.transpose(gradf), p) ):  \n",
        "    alpha = rho*alpha\n",
        "\n",
        "  #print('final step length:',alpha)\n",
        "  return alpha"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqjdHM3eaHYf"
      },
      "source": [
        "def compute_steplength_backtracking_scaled_direction(x,gradf,alpha_start, rho, gamma): #add appropriate arguments to the function \n",
        "  assert type(x) is np.ndarray and len(x) == 2 \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(alpha_start) is float and alpha_start>=0.\n",
        "  assert type(rho) is float and rho>=0.\n",
        "  assert type(gamma) is float and gamma>=0. \n",
        "  \n",
        "  alpha = alpha_start\n",
        "  p = -gradf\n",
        "  D_k = compute_D_k(x)\n",
        "  m = np.matmul(D_k,p)\n",
        "  while evalf(x)<evalf(x+alpha*m)+(np.matmul(np.matrix.transpose(gradf), m))*alpha*gamma:\n",
        "    alpha = alpha*rho\n",
        "  return alpha\n"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvLRu5s635ph"
      },
      "source": [
        "#line search type \n",
        "EXACT_LINE_SEARCH = 1\n",
        "BACKTRACKING_LINE_SEARCH = 2\n",
        "CONSTANT_STEP_LENGTH = 3"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejfX_z4DNcJq"
      },
      "source": [
        "def find_minimizer_gdscaling(start_x, tol, line_search_type, *args):\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "  #initialization for backtracking line search\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\n",
        "    alpha_start = args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        "  k = 0\n",
        "  while (np.linalg.norm(g_x) > tol):\n",
        "    D_k = compute_D_k(x)\n",
        "    if line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      step_length = compute_steplength_backtracking_scaled_direction(x,g_x, alpha_start, rho, gamma) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('BACKTRACKING LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\n",
        "      step_length = 1.0\n",
        "    else:  \n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "    \n",
        "     #call the new function you wrote to compute the steplength\n",
        "    x = np.subtract(x, np.multiply(step_length,np.matmul(D_k, g_x))) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    gradf = evalg(x)\n",
        "    g_x = evalg(x) #compute gradient at new point    \n",
        "  return x , evalf(x), k\n",
        "\n"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDPZUhwReekW"
      },
      "source": [
        "Que-3: \\\\\n",
        "I. **Constant Step Length**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhSvRsSHOPgC",
        "outputId": "a8aa702a-f044-4923-e899-67c3d522d94e"
      },
      "source": [
        "my_start_x = np.array([2.0, 2.0])\n",
        "my_tol= 1e-9\n",
        "\n",
        "opt_x, opt_f, iter = find_minimizer_gdscaling(my_start_x, my_tol, CONSTANT_STEP_LENGTH)\n",
        "print(\"Minimizer:\",opt_x)\n",
        "print(\"Minimum value:\",opt_f)\n",
        "print(\"Num of iterations:\",iter)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimizer: [0.         0.00304488]\n",
            "Minimum value: 3.4382653805813626e-13\n",
            "Num of iterations: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc7sFSWIewyv"
      },
      "source": [
        "II. **Backtracking Line Search**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0qtFsHKOsec",
        "outputId": "1ce865a0-094b-4359-b167-876e1d426935"
      },
      "source": [
        "alpha_start = 1.0\n",
        "rho = 0.5\n",
        "gamma = 0.5\n",
        "opt_x, opt_f, iter= find_minimizer_gdscaling(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH,alpha_start, rho, gamma )\n",
        "print(\"Minimizer:\",opt_x)\n",
        "print(\"Minimum value:\",opt_f)\n",
        "print(\"Num of iterations:\",iter)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimizer: [0.         0.00304488]\n",
            "Minimum value: 3.4382653805813626e-13\n",
            "Num of iterations: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLjZscsGg72c"
      },
      "source": [
        "##Observation\n",
        "\n",
        "The newton's method with constant step length and with backtracking line search both converge exactly in 16 iterations. The minimum function value in both cases is same and sufficiently close to the optimal value. For the minimizer the optimal value is [0, 0] . The first coordinate is 0 in both cases while second coordinate is 0.003 which can be improved further to precision, using high tolerance value. \\\\\n",
        "Both methods of line search behave the same in this case, because the backtracking line search returns step length value 1.0 at each iteration for this particular function, which is same as the constant step length 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKRNoshie9mb"
      },
      "source": [
        "Que-4: \\\\\n",
        "**I. Backtracking line search (without scaling)**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_minimizer(start_x, tol, line_search_type,*args):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0. \n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "  gradf = evalg(x)\n",
        "\n",
        "  #initialization for backtracking line search\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\n",
        "    alpha_start = args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        "\n",
        "  k = 0\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "    if line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      step_length = compute_steplength_backtracking(x,gradf, alpha_start,rho, gamma) #call the new function you wrote to compute the steplength\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\n",
        "      step_length = 1.0\n",
        "    else:  \n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "    \n",
        "    #implement the gradient descent steps here   \n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "    gradf = evalg(x)\n",
        "    #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  return x ,k"
      ],
      "metadata": {
        "id": "C02X1zNToRYW"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "_A0fWzBeTpac",
        "outputId": "2cf10174-187a-4e5d-a3c2-46d41241138b"
      },
      "source": [
        "opt_x, opt_f, iter = find_minimizer(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH,alpha_start, rho, gamma )\n",
        "print(\"Minimizer:\",opt_x)\n",
        "print(\"Minimum value:\",opt_f)\n",
        "print(\"Num of iterations:\",iter)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-94-da4773c05336>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mopt_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_minimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_start_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_tol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBACKTRACKING_LINE_SEARCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Minimizer:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopt_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Minimum value:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopt_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Num of iterations:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-93-034dcf0abe55>\u001b[0m in \u001b[0;36mfind_minimizer\u001b[0;34m(start_x, tol, line_search_type, *args)\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_x\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#continue as long as the norm of gradient is not close to zero upto a tolerance tol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mline_search_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mBACKTRACKING_LINE_SEARCH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m       \u001b[0mstep_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_steplength_backtracking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgradf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#call the new function you wrote to compute the steplength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mline_search_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mCONSTANT_STEP_LENGTH\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#do a gradient descent with constant step length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0mstep_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-87-410891e5460e>\u001b[0m in \u001b[0;36mcompute_steplength_backtracking\u001b[0;34m(x, gradf, alpha_start, rho, gamma)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mgradf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m#implement the backtracking line search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0;32mwhile\u001b[0m \u001b[0mevalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mevalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m#while evalf(x + alpha*p) > evalf(x) + gamma * alpha* (np.matmul(np.matrix.transpose(gradf), p) ):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I had to interrupt the code as it was running for quite long time."
      ],
      "metadata": {
        "id": "IkxKRAfKpt63"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf1bwnmtfPPE"
      },
      "source": [
        "II. **Backtracking Line-search(with scaling, using diagonal matrix)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0pYHnNeXYSW"
      },
      "source": [
        "def find_minimizer_gdscaling(start_x, tol, line_search_type, *args):\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "  #initialization for backtracking line search\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\n",
        "    alpha_start = args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        "  k = 0\n",
        "  while (np.linalg.norm(g_x) > tol):\n",
        "    D_k = compute_D_k(x)\n",
        "    if line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      step_length = compute_steplength_backtracking_scaled_direction(x,g_x, alpha_start, rho, gamma) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('BACKTRACKING LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\n",
        "      step_length = 1\n",
        "    else:  \n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "    \n",
        "     #call the new function you wrote to compute the steplength\n",
        "    x = np.subtract(x, np.multiply(step_length,np.matmul(D_k, g_x))) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "  return x , evalf(x), k\n",
        "\n"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5zQHS6Cd-jp",
        "outputId": "fea6cb7f-3b42-4118-91d8-845e0f382aea"
      },
      "source": [
        "my_start_x = np.array([2.0, 2.0])\n",
        "my_tol= 1e-9\n",
        "alpha_start = 1.0\n",
        "rho = 0.5\n",
        "gamma = 0.5\n",
        "opt_x, opt_f, iter = find_minimizer_gdscaling(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH,alpha_start, rho, gamma )\n",
        "print(\"Minimizer:\",opt_x)\n",
        "print(\"Minimum value:\",opt_f)\n",
        "print(\"Num of iterations:\",iter)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimizer: [0.         0.00304488]\n",
            "Minimum value: 3.4382653805813626e-13\n",
            "Num of iterations: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPknEvU9g_df"
      },
      "source": [
        "##Observation:\n",
        "The Newton's method is equivalent to scaling with a diagonal matrix for this function, since the hessian is itself a diagonal matrix. Let D = $diag(d_{1}, d_{2},... d_{k})$ be a Diagonal matrix. Then $D^{-1} = diag(1/d_{1},1/ d_{2},... 1/d_{k})$, which is same as the matrix used for diagonal scaling. Thus the diagonal scaling method behaves exactly the same as the two variants of Newton's method."
      ]
    }
  ]
}